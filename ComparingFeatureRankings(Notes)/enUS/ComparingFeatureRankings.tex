\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{listings}  % For code listings
\usepackage{xcolor}    % For color in code listings
\usepackage{graphicx}  % For including images
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

% Adjust the margins here
\geometry{
  a4paper,         % Paper size
  left=25mm,       % Left margin
  right=25mm,      % Right margin
  top=25mm,        % Top margin
  bottom=25mm      % Bottom margin
}

\title{Comparison of Ranking Algorithms}
\author{}
\date{}

% Python code style for listings
\lstdefinestyle{pythonstyle}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{gray},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  backgroundcolor=\color{lightgray!20},
  frame=single,
  captionpos=b
}

% R code style for listings
\lstdefinestyle{rstyle}{
  language=R,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{gray},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  backgroundcolor=\color{lightgray!20},
  frame=single,
  captionpos=b
}

\begin{document}

\maketitle

To determine whether the rankings produced by different algorithms are significantly different, you can adopt statistical methods suited for comparing ranks. Here’s how you might approach it as a statistician:

\begin{enumerate}
    \item \textbf{Kendall’s Tau or Spearman’s Rank Correlation}:  
    \begin{itemize}
        \item These methods assess the correlation between rankings. If you have two ranking algorithms, you can compute either \textbf{Kendall's Tau} or \textbf{Spearman's Rank Correlation Coefficient} to see how similar or different the rankings are. These coefficients range from -1 (completely inverse rankings) to 1 (identical rankings).
        \item A significant p-value from a hypothesis test (associated with the correlation) would indicate that the rankings are statistically different.
    \end{itemize}

    \item \textbf{Permutation Test}:  
    \begin{itemize}
        \item This non-parametric approach can help you assess whether the observed difference in ranks (or their correlation) is due to chance. You can randomly shuffle the rankings many times and compare the observed difference (using a correlation metric like Kendall’s Tau or Spearman’s) to the distribution of differences under random ranking permutations.
        \item If the observed difference is extreme compared to this null distribution, it suggests that the rankings are significantly different.
    \end{itemize}

    \item \textbf{Rank-Biserial Correlation}:  
    \begin{itemize}
        \item If you have two groups of rankings, you could use the \textbf{Rank-Biserial Correlation} to measure the degree of separation between two algorithms’ rankings. This could be useful in scenarios where you want to test if one ranking algorithm consistently ranks certain features higher than the other.
    \end{itemize}

    \item \textbf{Consensus Ranking Methods}:  
    \begin{itemize}
        \item You might compute a \textbf{consensus ranking} (e.g., using Borda Count or similar) and then test how far each algorithm's ranking deviates from this consensus using measures like distance metrics (e.g., Manhattan distance or Euclidean distance). You could then test these distances for significance using statistical tests.
    \end{itemize}

    \item \textbf{Friedman Test and Post-Hoc Analysis}:  
    \begin{itemize}
        \item If you are comparing more than two ranking algorithms, you could use the \textbf{Friedman Test}, a non-parametric test for detecting differences across multiple algorithms. If the test is significant, you can follow up with post-hoc pairwise tests, such as the Nemenyi test, to determine which rankings differ.
    \end{itemize}

    \item \textbf{Bootstrapping}:  
    \begin{itemize}
        \item You can resample your data and reapply the ranking algorithms to create bootstrapped distributions of the rankings. By comparing the bootstrapped distributions, you can determine if there are significant differences in how the algorithms rank the features.
    \end{itemize}
\end{enumerate}

Each of these methods allows you to statistically evaluate whether there are significant differences between the rankings from the various algorithms. Would you like more details or examples of how to apply any of these methods?

The most commonly used methods for comparing rankings of features in data science problems tend to be \textbf{Kendall's Tau} and \textbf{Spearman's Rank Correlation}. These methods are widely used because they provide straightforward measures of rank similarity or dissimilarity and are easy to interpret.

\section*{Why These Methods Are Common:}
\begin{enumerate}
    \item \textbf{Simplicity}: Both are relatively easy to compute and understand. They provide a direct measure of how similar or different two sets of rankings are.
    \item \textbf{Hypothesis Testing}: Both Kendall's Tau and Spearman's Rank Correlation offer hypothesis tests with p-values, which makes it simple to assess whether the difference between rankings is statistically significant.
    \item \textbf{Interpretation}: The correlation coefficients from these methods are intuitive: values close to 1 mean rankings are highly similar, and values close to -1 mean rankings are highly dissimilar. This makes them appealing for practitioners.
    \item \textbf{Broad Applicability}: These methods work well even when the distributions of the features are non-normal or the relationships are nonlinear, which is often the case in feature rankings.
\end{enumerate}

In the context of comparing multiple ranking algorithms, the \textbf{Friedman Test} is also popular for comparing more than two rankings at once, but \textbf{Kendall’s Tau} and \textbf{Spearman’s} are the go-to methods for pairwise comparisons.

Here is an example of how you can use Python to run \textbf{Kendall's Tau} and \textbf{Spearman's Rank Correlation} tests to compare two sets of rankings. This code uses the \texttt{scipy.stats} module for calculating these correlation coefficients and performing the statistical tests.

\begin{lstlisting}[style=pythonstyle, caption={Python code for Kendall's Tau and Spearman's Rank Correlation}]
    import numpy as np
    from scipy.stats import kendalltau, spearmanr
    
    # Example rankings from two different algorithms
    ranking_algo1 = [1, 2, 3, 4, 5]
    ranking_algo2 = [2, 1, 4, 3, 5]
    
    # Kendall's Tau test
    kendall_tau_corr, kendall_tau_pvalue = kendalltau(ranking_algo1, ranking_algo2)
    print(f"Kendall's Tau Correlation: {kendall_tau_corr}, p-value: {kendall_tau_pvalue}")
    
    # Spearman's Rank Correlation test
    spearman_corr, spearman_pvalue = spearmanr(ranking_algo1, ranking_algo2)
    print(f"Spearman's Rank Correlation: {spearman_corr}, p-value: {spearman_pvalue}")
\end{lstlisting}
    

\section*{Explanation:}
\begin{itemize}
    \item \texttt{kendalltau()} computes the Kendall's Tau correlation coefficient and its associated p-value. The correlation measures the ordinal association between the rankings.
    \item \texttt{spearmanr()} computes Spearman’s Rank Correlation and provides the correlation coefficient and p-value. Spearman’s method is similar to Pearson’s correlation but for ranked data.
\end{itemize}

\section*{Output:}
\begin{itemize}
    \item \textbf{Correlation Coefficient}: A value between -1 and 1, where:
    \begin{itemize}
        \item 1 means the rankings are identical.
        \item 0 means no correlation.
        \item -1 means the rankings are inversely related.
    \end{itemize}
    \item \textbf{p-value}: The significance level of the test. If the p-value is below a significance threshold (commonly 0.05), you reject the null hypothesis, meaning that the rankings are statistically different.
\end{itemize}

This code can be modified to compare multiple rankings or used within a loop to evaluate several algorithms. 

\section*{Example: Applying the Friedman Test and Post-Hoc Analysis}

In this example, we'll demonstrate how to perform the Friedman test to compare multiple ranking algorithms and, if significant differences are found, apply a post-hoc test for pairwise comparisons. We'll use both Python and R for this demonstration, including code for visualizing the results.

\subsection*{Dataset}

Suppose we have the rankings of features produced by three different algorithms across five datasets. The rankings are as follows:

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Dataset} & \textbf{Algorithm 1} & \textbf{Algorithm 2} & \textbf{Algorithm 3} \\
\hline
Dataset 1 & 1 & 2 & 1 \\
Dataset 2 & 2 & 1 & 2 \\
Dataset 3 & 3 & 3 & 3 \\
Dataset 4 & 4 & 4 & 5 \\
Dataset 5 & 5 & 5 & 4 \\
\hline
\end{tabular}
\caption{Rankings of features by three algorithms across five datasets}
\end{table}

\subsection*{Python Implementation}

\begin{lstlisting}[style=pythonstyle, caption={Python code for Friedman test and post-hoc analysis}]
import numpy as np
from scipy.stats import friedmanchisquare
import scikit_posthocs as sp
import pandas as pd
import matplotlib.pyplot as plt

# Rankings data
data = {
    'Algorithm1': [1, 2, 3, 4, 5],
    'Algorithm2': [2, 1, 3, 4, 5],
    'Algorithm3': [1, 2, 3, 5, 4]
}

df = pd.DataFrame(data, index=['Dataset1', 'Dataset2', 'Dataset3', 'Dataset4', 'Dataset5'])

# Perform Friedman test
stat, p = friedmanchisquare(df['Algorithm1'], df['Algorithm2'], df['Algorithm3'])
print(f'Friedman test statistic: {stat:.3f}, p-value: {p:.3f}')

# Check if the result is significant
if p < 0.05:
    print('Significant differences found. Proceeding to post-hoc test.')
    
    # Perform Nemenyi post-hoc test
    nemenyi = sp.posthoc_nemenyi_friedman(df.values)
    nemenyi.index = ['Algorithm1', 'Algorithm2', 'Algorithm3']
    nemenyi.columns = ['Algorithm1', 'Algorithm2', 'Algorithm3']
    print(nemenyi)
    
    # Visualization of the post-hoc test results
    sp.sign_plot(nemenyi, alpha=0.05)
    plt.title('Nemenyi Post-hoc Test Results')
    plt.show()
else:
    print('No significant differences found.')
\end{lstlisting}

\subsubsection*{Explanation}

\begin{itemize}
    \item \texttt{friedmanchisquare()} performs the Friedman test on the rankings from the three algorithms.
    \item If the p-value is less than 0.05, we conclude that there are significant differences among the algorithms.
    \item We then perform the Nemenyi post-hoc test using \texttt{posthoc\_nemenyi\_friedman()} from the \texttt{scikit\_posthocs} library.
    \item The \texttt{sign\_plot()} function visualizes the significant differences between pairs of algorithms.
\end{itemize}

\subsection*{R Implementation}

\begin{lstlisting}[style=rstyle, caption={R code for Friedman test and post-hoc analysis}]
# Install necessary packages if not already installed
# install.packages("PMCMRplus")
# install.packages("ggplot2")
# install.packages("reshape2")
# install.packages("multcompView")

library(PMCMRplus)
library(ggplot2)
library(reshape2)
library(multcompView)

# Rankings data
Algorithm1 <- c(1, 2, 3, 4, 5)
Algorithm2 <- c(2, 1, 3, 4, 5)
Algorithm3 <- c(1, 2, 3, 5, 4)
data <- data.frame(Algorithm1, Algorithm2, Algorithm3)
rownames(data) <- c("Dataset1", "Dataset2", "Dataset3", "Dataset4", "Dataset5")

# Perform Friedman test
friedman_result <- friedman.test(as.matrix(data))
print(friedman_result)

# Check if the result is significant
if (friedman_result$p.value < 0.05) {
  print("Significant differences found. Proceeding to post-hoc test.")
  
  # Perform Nemenyi post-hoc test
  posthoc_result <- posthoc.friedman.nemenyi.test(as.matrix(data))
  print(posthoc_result)
  
  # Visualization of the post-hoc test results
  p_values <- as.data.frame(posthoc_result$p.value)
  p_values$Algorithm <- rownames(p_values)
  melt_pvalues <- melt(p_values, id.vars = 'Algorithm')

  ggplot(melt_pvalues, aes(x = Algorithm, y = variable, fill = value)) +
    geom_tile() +
    geom_text(aes(label = sprintf("%.3f", value)), color = "black") +
    scale_fill_gradient(low = "white", high = "red") +
    theme_minimal() +
    labs(title = "Nemenyi Post-hoc Test Results", x = "", y = "") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
} else {
  print("No significant differences found.")
}
\end{lstlisting}

\subsubsection*{Explanation}

\begin{itemize}
    \item \texttt{friedman.test()} performs the Friedman test on the rankings.
    \item If the p-value is less than 0.05, we proceed with the Nemenyi post-hoc test using \texttt{posthoc.friedman.nemenyi.test()} from the \texttt{PMCMRplus} package.
    \item The visualization uses \texttt{ggplot2} to create a heatmap of the p-values between algorithm pairs.
\end{itemize}

\subsection*{Results Interpretation}

In both Python and R implementations, we first perform the Friedman test to see if there are statistically significant differences among the algorithms' rankings. If significant differences are found (p-value $<$ 0.05), we proceed with the Nemenyi post-hoc test to identify which pairs of algorithms differ.

\subsubsection*{Visualization}

The visualizations help in quickly identifying significant differences:

\begin{itemize}
    \item In Python, the \texttt{sign\_plot()} function generates a plot where significant differences are indicated.
    \item In R, the heatmap displays the p-values between pairs of algorithms, with lower p-values (e.g., $<$ 0.05) indicating significant differences.
\end{itemize}

\subsection*{Conclusion}

This example demonstrates how to perform a Friedman test and follow up with a post-hoc analysis if necessary. Including visualization aids in interpreting the results and communicating findings effectively.

\end{document}
